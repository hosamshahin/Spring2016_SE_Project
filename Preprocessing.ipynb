{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Tokenizing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She', 'looked', 'at', '', '', 'her', \"father's\", 'arm-chair.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note: there are three spaces between \"at\" and \"her\" to make the example more\n",
    "# realistic (texts are frequently plagued by such idiosyncracies)\n",
    "text = \"She looked at   her father's arm-chair.\"\n",
    "\n",
    "text_fr = \"Qu'est-ce que c'est?\"\n",
    "\n",
    "# tokenize on spaces\n",
    "text.split(' ')\n",
    "# Out[3]: ['She', 'looked', 'at', '', '', 'her', \"father's\", 'arm-chair.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Qu'est-ce\", 'que', \"c'est?\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_fr.split(' ')\n",
    "# Out[4]: [\"Qu'est-ce\", 'que', \"c'est?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She', 'looked', 'at', 'her', 'father', 'arm', 'chair']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scikit-learn\n",
    "# note that CountVectorizer discards \"words\" that contain only one character, such as \"s\"\n",
    "# CountVectorizer also transforms all words into lowercase\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "CountVectorizer().build_tokenizer()(text)\n",
    "# Out[6]: ['She', 'looked', 'at', 'her', 'father', 'arm', 'chair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Qu', 'est', 'ce', 'que', 'est']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVectorizer().build_tokenizer()(text_fr)\n",
    "# Out[7]: ['Qu', 'est', 'ce', 'que', 'est']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She', 'looked', 'at', 'her', 'father', \"'s\", 'arm-chair', '.']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk word_tokenize uses the TreebankWordTokenizer and needs to be given\n",
    "# a single sentence at a time.\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(text)\n",
    "# Out[9]: ['She', 'looked', 'at', 'her', 'father', \"'s\", 'arm-chair', '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Stemming</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'wald', u'wald', u'wald', u'wald']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import GermanStemmer\n",
    "\n",
    "stemmer = GermanStemmer()\n",
    "\n",
    "# note that the stem function works one word at a time\n",
    "# words = [\"Wald\", \"Walde\", \"Wälder\", \"Wäldern\", \"Waldes\",\"Walds\"]\n",
    "words = [\"Wald\", \"Walde\", \"Waldes\",\"Walds\"]\n",
    "\n",
    "[stemmer.stem(w) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'waldi'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that the stemming algorithm \"understands\" grammar to some extent and that if \"Waldi\" were to \n",
    "# appear in a text, it would not be stemmed.\n",
    "stemmer.stem(\"Waldi\")\n",
    "# Out[23]: 'waldi'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Chunking Splitting</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Crebillon_TR-V-1703-Idomenee.txt',\n",
       " 'Crebillon_TR-V-1707-Atree.txt',\n",
       " 'Crebillon_TR-V-1708-Electre.txt',\n",
       " 'Crebillon_TR-V-1711-Rhadamisthe.txt',\n",
       " 'Crebillon_TR-V-1717-Semiramis.txt']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# plays are in the directory data/french-tragedy\n",
    "# gather all the filenames, sorted alphabetically\n",
    "corpus_path = os.path.join('data', 'data', 'french-tragedy')\n",
    "\n",
    "# look at the first few filenames\n",
    "# (we are sorting because different operating systems may list files in different orders)\n",
    "sorted(os.listdir(corpus_path))[0:5]\n",
    "# Out[27]: \n",
    "# ['Crebillon_TR-V-1703-Idomenee.txt',\n",
    "#  'Crebillon_TR-V-1707-Atree.txt',\n",
    "#  'Crebillon_TR-V-1708-Electre.txt',\n",
    "#  'Crebillon_TR-V-1711-Rhadamisthe.txt',\n",
    "#  'Crebillon_TR-V-1717-Semiramis.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we will need the entire path, e.g., 'data/Crebillon_TR-V-1703-Idomenee.txt'\n",
    "# rather than just 'Crebillon_TR-V-1703-Idomenee.txt' alone.\n",
    "tragedy_filenames = [os.path.join(corpus_path, fn) for fn in sorted(os.listdir(corpus_path))]\n",
    "\n",
    "# alternatively, using the Python standard library package 'glob'\n",
    "import glob\n",
    "\n",
    "tragedy_filenames = glob.glob(corpus_path + os.sep + '*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_text(filename, n_words):\n",
    "  \"\"\"Split a text into chunks approximately `n_words` words in length.\"\"\"\n",
    "  input = open(filename, 'r')\n",
    "  words = input.read().split(' ')\n",
    "  input.close()\n",
    "  chunks = []\n",
    "  current_chunk_words = []\n",
    "  current_chunk_word_count = 0\n",
    "  for word in words:\n",
    "      current_chunk_words.append(word)\n",
    "      current_chunk_word_count += 1\n",
    "      if current_chunk_word_count == n_words:\n",
    "          chunks.append(' '.join(current_chunk_words))\n",
    "          current_chunk_words = []\n",
    "          current_chunk_word_count = 0\n",
    "  chunks.append(' '.join(current_chunk_words) )\n",
    "  return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tragedy_filenames = [os.path.join(corpus_path, fn) for fn in sorted(os.listdir(corpus_path))]\n",
    "\n",
    "# alternatively, using glob\n",
    "tragedy_filenames = glob.glob(corpus_path + os.sep + '*.txt')\n",
    "\n",
    "# for consistency across platforms (Linux, OS X, Windows) we must sort the filenames\n",
    "tragedy_filenames.sort()\n",
    "\n",
    "chunk_length = 1000\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for filename in tragedy_filenames:\n",
    "      chunk_counter = 0\n",
    "      texts = split_text(filename, chunk_length)\n",
    "      for text in texts:\n",
    "          chunk = {'text': text, 'number': chunk_counter, 'filename': filename}\n",
    "          chunks.append(chunk)\n",
    "          chunk_counter += 1\n",
    "   \n",
    "# we started with this many files ...\n",
    "len(tragedy_filenames)\n",
    "# Out[38]: 59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for file in tragedy_filenames:\n",
    "#     print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2740"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ... and now we have this many\n",
    "len(chunks)\n",
    "# Out[39]: 2740"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'abaisse' u'abaissement' u'abaisser' ..., u'\\xf4tera' u'\\xf4tez'\n",
      " u'\\xf4t\\xe9']\n"
     ]
    }
   ],
   "source": [
    "# from the triples we can create a document-term matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=5, max_df=.95)\n",
    "\n",
    "dtm = vectorizer.fit_transform([c['text'] for c in chunks])\n",
    "\n",
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make sure the directory exists (not working!!!)\n",
    "output_dir = '/tmp/'\n",
    "\n",
    "for chunk in chunks:\n",
    "      basename = os.path.basename(chunk['filename'])\n",
    "      fn = os.path.join(output_dir,\n",
    "                        \"{}{:04d}\".format(basename, chunk['number']))\n",
    "      with open(fn, 'w') as f:\n",
    "          f.write(chunk['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Grouping</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Voltaire_TR-V-1764-Olympie.txt'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in every filename the author's last name is followed by an underscore ('_'),\n",
    "# for example: Voltaire_TR-V-1764-Olympie.txt\n",
    "# os.path.basename(...) gets us the filename from a path, e.g.,\n",
    "os.path.basename('french-tragedy/Voltaire_TR-V-1764-Olympie.txt')\n",
    "# Out[54]: 'Voltaire_TR-V-1764-Olympie.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Voltaire', 'TR-V-1764-Olympie.txt']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the split method we can break up the string on the underscore ('_')\n",
    "os.path.basename('french-tragedy/Voltaire_TR-V-1764-Olympie.txt').split('_')\n",
    "# Out[55]: ['Voltaire', 'TR-V-1764-Olympie.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Voltaire'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# putting these two steps together\n",
    "author = os.path.basename('french-tragedy/Voltaire_TR-V-1764-Olympie.txt').split('_')[0]\n",
    "\n",
    "author\n",
    "# Out[57]: 'Voltaire'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Crebillon',\n",
       " 'Crebillon',\n",
       " 'Crebillon',\n",
       " 'Crebillon',\n",
       " 'Crebillon',\n",
       " 'Crebillon',\n",
       " 'Crebillon',\n",
       " 'Crebillon',\n",
       " 'Crebillon',\n",
       " 'PCorneille',\n",
       " 'PCorneille',\n",
       " 'PCorneille',\n",
       " 'PCorneille',\n",
       " 'PCorneille',\n",
       " 'PCorneille',\n",
       " 'PCorneille',\n",
       " 'PCorneille',\n",
       " 'PCorneille',\n",
       " 'PCorneille',\n",
       " 'PCorneille',\n",
       " 'PCorneille',\n",
       " 'PCorneille',\n",
       " 'PCorneille',\n",
       " 'PCorneille',\n",
       " 'PCorneille',\n",
       " 'PCorneille',\n",
       " 'PCorneille',\n",
       " 'PCorneille',\n",
       " 'PCorneille',\n",
       " 'Racine',\n",
       " 'Racine',\n",
       " 'Racine',\n",
       " 'Racine',\n",
       " 'Racine',\n",
       " 'Racine',\n",
       " 'Racine',\n",
       " 'Racine',\n",
       " 'Racine',\n",
       " 'Racine',\n",
       " 'Racine',\n",
       " 'Voltaire',\n",
       " 'Voltaire',\n",
       " 'Voltaire',\n",
       " 'Voltaire',\n",
       " 'Voltaire',\n",
       " 'Voltaire',\n",
       " 'Voltaire',\n",
       " 'Voltaire',\n",
       " 'Voltaire',\n",
       " 'Voltaire',\n",
       " 'Voltaire',\n",
       " 'Voltaire',\n",
       " 'Voltaire',\n",
       " 'Voltaire',\n",
       " 'Voltaire',\n",
       " 'Voltaire',\n",
       " 'Voltaire',\n",
       " 'Voltaire',\n",
       " 'Voltaire']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and for all the authors\n",
    "authors = [os.path.basename(filename).split('_')[0] for filename in tragedy_filenames]\n",
    "\n",
    "authors\n",
    "# Out[59]: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Crebillon', 'PCorneille', 'Racine', 'Voltaire'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to ignore duplicates we can transform the list into a set (which only records unique elements)\n",
    "set(authors)\n",
    "# Out[60]: {'Crebillon', 'PCorneille', 'Racine', 'Voltaire'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Crebillon', 'PCorneille', 'Racine', 'Voltaire']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as there is no guarantee about the ordering in a set (or a dictionary) we will typically\n",
    "# first drop duplicates and then save our unique names as a sorted list. Because there are\n",
    "# no duplicates in this list, we can be confident that the ordering is the same every time.\n",
    "sorted(set(authors))\n",
    "# Out[61]: ['Crebillon', 'PCorneille', 'Racine', 'Voltaire']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Crebillon'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and we have a way of finding which indexes in authors correspond to each author using array indexing\n",
    "authors = np.array(authors)  # convert from a Python list to a NumPy array\n",
    "\n",
    "first_author = sorted(set(authors))[0]\n",
    "\n",
    "first_author\n",
    "# Out[64]: 'Crebillon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False], dtype=bool)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors == first_author\n",
    "# Out[65]: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8]),)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nonzero(authors == first_author)  # if we want the actual indexes\n",
    "# Out[66]: (array([0, 1, 2, 3, 4, 5, 6, 7, 8]),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Crebillon', 'Crebillon', 'Crebillon', 'Crebillon', 'Crebillon',\n",
       "       'Crebillon', 'Crebillon', 'Crebillon', 'Crebillon'], \n",
       "      dtype='|S10')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors[np.nonzero(authors == first_author)]\n",
    "# Out[67]: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['PCorneille', 'PCorneille', 'PCorneille', 'PCorneille',\n",
       "       'PCorneille', 'PCorneille', 'PCorneille', 'PCorneille',\n",
       "       'PCorneille', 'PCorneille', 'PCorneille', 'PCorneille',\n",
       "       'PCorneille', 'PCorneille', 'PCorneille', 'PCorneille',\n",
       "       'PCorneille', 'PCorneille', 'PCorneille', 'PCorneille', 'Racine',\n",
       "       'Racine', 'Racine', 'Racine', 'Racine', 'Racine', 'Racine',\n",
       "       'Racine', 'Racine', 'Racine', 'Racine', 'Voltaire', 'Voltaire',\n",
       "       'Voltaire', 'Voltaire', 'Voltaire', 'Voltaire', 'Voltaire',\n",
       "       'Voltaire', 'Voltaire', 'Voltaire', 'Voltaire', 'Voltaire',\n",
       "       'Voltaire', 'Voltaire', 'Voltaire', 'Voltaire', 'Voltaire',\n",
       "       'Voltaire', 'Voltaire'], \n",
       "      dtype='|S10')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alternatively, we can find those indexes of texts *not* written by `first_author`\n",
    "authors[authors != first_author]\n",
    "# Out[68]: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first get a document-term-matrix of word frequencies for our corpus\n",
    "vectorizer = CountVectorizer(input='filename')\n",
    "\n",
    "# fit_transform returns a sparse matrix (which uses less memory)\n",
    "# but we want to work with a normal numpy array.\n",
    "dtm = vectorizer.fit_transform(tragedy_filenames).toarray()\n",
    "        \n",
    "vocab = np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the top 10 higest ranked words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import nltk\n",
    "# import numpy as np\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# filenames = ['Austen_Emma.txt', 'Austen_Pride.txt', 'Austen_Sense.txt', 'CBronte_Jane.txt', 'CBronte_Professor.txt',\n",
    "#  'CBronte_Villette.txt']\n",
    "\n",
    "# filenames_with_path = [os.path.join(corpus_path, fn) for fn in filenames]\n",
    "\n",
    "# # these texts have underscores ('_') that indicate italics; remove them.\n",
    "# raw_texts = []\n",
    "# for fn in filenames_with_path:\n",
    "#     with open(fn) as f:\n",
    "#         text = f.read()\n",
    "#         text = text.replace('_', '')  # remove underscores (italics)\n",
    "#         raw_texts.append(text)\n",
    "\n",
    "# vectorizer = CountVectorizer(input='content')\n",
    "# dtm = vectorizer.fit_transform(raw_texts).toarray()\n",
    "\n",
    "# vocab = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "\n",
    "# # normalize counts to rates per 1000 words\n",
    "# rates = 1000 * dtm / np.sum(dtm, axis=1, keepdims=True)\n",
    "# # print the top 10 words along with their rates and the difference\n",
    "# vocab[ranking][0:10]\n",
    "\n",
    "# # indices so we can refer to the rows for the relevant author\n",
    "# austen_indices, cbronte_indices = [], []\n",
    "\n",
    "# for index, fn in enumerate(filenames):\n",
    "#     if \"Austen\" in fn:\n",
    "#         austen_indices.append(index)\n",
    "#     elif \"CBronte\" in fn:\n",
    "#         cbronte_indices.append(index)\n",
    "\n",
    "# # this kind of slicing should be familiar if you've used R or Octave/Matlab\n",
    "# austen_rates = rates[austen_indices, :]\n",
    "\n",
    "# cbronte_rates = rates[cbronte_indices, :]\n",
    "\n",
    "# # np.mean(..., axis=0) calculates the column-wise mean\n",
    "# austen_rates_avg = np.mean(austen_rates, axis=0)\n",
    "\n",
    "# cbronte_rates_avg = np.mean(cbronte_rates, axis=0)\n",
    "\n",
    "# # calculate absolute value because we only care about the magnitude of the difference\n",
    "# keyness = np.abs(austen_rates_avg - cbronte_rates_avg)\n",
    "\n",
    "# ranking = np.argsort(keyness)[::-1]  # from highest to lowest; [::-1] reverses order in Python sequences\n",
    "\n",
    "# # print the top 10 words along with their rates and the difference\n",
    "# vocab[ranking][0:10]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "authors = np.array([os.path.basename(filename).split('_')[0] for filename in tragedy_filenames])\n",
    "\n",
    "# allocate an empty array to store our aggregated word frequencies\n",
    "authors_unique = sorted(set(authors))\n",
    "\n",
    "dtm_authors = np.zeros((len(authors_unique), len(vocab)))\n",
    "\n",
    "for i, author in enumerate(authors_unique):\n",
    "     dtm_authors[i, :] = np.sum(dtm[authors==author, :], axis=0)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Grouping using Pandas</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>16597</th>\n",
       "      <th>16598</th>\n",
       "      <th>16599</th>\n",
       "      <th>16600</th>\n",
       "      <th>16601</th>\n",
       "      <th>16602</th>\n",
       "      <th>16603</th>\n",
       "      <th>16604</th>\n",
       "      <th>16605</th>\n",
       "      <th>16606</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Crebillon</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCorneille</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Racine</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Voltaire</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 16607 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0      1      2      3      4      5      6      7      8      \\\n",
       "Crebillon       0      0      0      0      0      0      0      0      0   \n",
       "PCorneille      1      1      2      2      1      1      1      1      1   \n",
       "Racine          0      0      0      0      0      0      0      0      0   \n",
       "Voltaire        0      0      0      0      0      0      0      0      0   \n",
       "\n",
       "            9      ...    16597  16598  16599  16600  16601  16602  16603  \\\n",
       "Crebillon       0  ...        1      0      0      1      0      0      2   \n",
       "PCorneille      1  ...        1      2      0      1      1      2     16   \n",
       "Racine          0  ...        1      0      0      0      0      0      2   \n",
       "Voltaire        0  ...        2      0      1      1      0      1      5   \n",
       "\n",
       "            16604  16605  16606  \n",
       "Crebillon       0      2      0  \n",
       "PCorneille      2     10      4  \n",
       "Racine          0      1      0  \n",
       "Voltaire        1      1      0  \n",
       "\n",
       "[4 rows x 16607 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "authors = [os.path.basename(filename).split('_')[0] for filename in tragedy_filenames]\n",
    "\n",
    "dtm_authors = pandas.DataFrame(dtm).groupby(authors).sum()\n",
    "\n",
    "dtm_authors.head()\n",
    "# dtm_authors = dtm_authors.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ...,  0,  2,  0],\n",
       "       [ 1,  1,  2, ...,  2, 10,  4],\n",
       "       [ 0,  0,  0, ...,  0,  1,  0],\n",
       "       [ 0,  0,  0, ...,  1,  1,  0]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projectNames = dtm_authors.index.values\n",
    "dtm_authors = dtm_authors.values\n",
    "dtm_authors[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Crebillon', 'PCorneille', 'Racine', 'Voltaire'], dtype=object)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projectNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
